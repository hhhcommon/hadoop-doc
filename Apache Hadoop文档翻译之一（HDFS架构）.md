> Apache Hadoop项目为高可用、可扩展、分布式计算开发开源软件。Apache Hadoop软件库是一个平台，它使用简单的编程模型让跨机器上大数据量的分布式计算变得简单。
它旨在从单个服务器扩展到数千台计算机，每台计算机都提供本地计算和存储。库本身被设计用来在软件层面检测和处理故障，而不是依赖硬件来提供高可用性，因此，在计算机集群之上提供高可用性服务，每个计算机都可能容易出现故障。

# 介绍
HDFS是一个被设计用来运行在商用机器上的分布式文件系统。它跟现有的分布式文件系统有很多相似之处，但是，区别也很大。HDFS容错率高，并且被设计为部署在廉价机器上。HDFS提供对应用程序数据的高吞吐量访问，适用于具有大型数据集的应用程序。HDFS放宽了一些POSIX要求，以实现对文件系统数据的流式访问。HDFS最开始被构建是用来作为Nutch搜索引擎项目的基础设施。HDFS是Apache hadoop核心项目的一部分。项目地址：[http://hadoop.apache.org/][1]

# 假设和目标
## 硬件故障
硬件故障很常见。一个HDFS示例应该是包含成百上千太服务器，每台服务器保存文件系统的部分数据。事实上，存在大量组件并且每个组件都有可能出现故障，这意外着有些组件可能一直无法正常工作。因此，检测故障并且自动从故障中快速恢复是HDFS的核心架构目标。
## 流数据访问
在HDFS上运行的程序需要对其数据集进行流式访问。它们不是运行在普通文件系统上的普通应用程序。HDFS被设计更多是作为批处理来使用，而不是用户的交互式使用。重点是数据访问的高吞吐量而不是数据访问的低延迟。POSIX强加了许多针对HDFS的应用程序不需要的硬性要求。
## 大数据集
运行在HDFS上的程序用户大量的数据集。HDFS中，一个普通文件可以达到千兆到太字节。因此，HDFS调整为支持大文件。它应该提供高聚合数据带宽并扩展到单个集群中的数百个节点。 它应该在单个实例中支持数千万个文件。

## 简单的一致性模型
HDFS程序需要一个一次写入多次读取的文件访问模型。一次创建、写入和关闭的文件，除了追加和截断之外，不需要更改。可以在文件尾部追加内容，但是不能在任意位置进行修改。这种假设简化了一致性问题，并且提高了数据访问的吞吐量。这种模型非常适合Mapreduce程序以及网络爬虫。
## 移动计算比移动数据代价更小
当计算所需要的数据距离计算越近时，计算的效率越高，当数据量越大时，这个效率提升更明显。越少的网络阻塞越能提高系统整体的吞吐量。这样，我们可以认为，将计算迁移到数据附近比将数据迁移到计算附近更高效。HHDFS为应用程序提供了接口，使其自身更靠近数据所在的位置。

## 跨异构硬件和软件平台的可移植性
HDFS被设计成易于从一个平台移植到另一个平台。这有助于HDFS的普及。

# Namenode和DataNodes
HDFS有一个主从架构。一个HDFS集群包含一个Namenode,Namenode管理文件系统的命名空间以及调整客户端对文件的访问。此外，还有一定数量的Datanode,集群中通常一个节点一个Datanode，用来管理其运行节点上的存储。HDFS公开文件系统命名空间，并允许用户数据存储在文件中。集群内部，一个文件被分成一个或多个blocks并且这些blocks存储在一些Datanode中。Namenode执行文件系统命名空间操作，例如打开、关闭、重命名文件和文件夹。Namenode还决定blocks到Datanode的映射。Datanode负责服务于系统客户端的读和写。DataNode还根据NameNode的指令执行块创建，删除和复制。
![IMAGE](quiver-image-url/077C4377C67276A5BEA581807A80F3FD.jpg =874x604)

[1]: http://hadoop.apache.org/


